{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 🎉 Welcome to the basics of working with genetic data! 🎉\n",
    "\n",
    "---\n",
    "\n",
    "### **Let's get started!** We will be going over some of the foundational quality control measures and analyses in population genetics. Here's a roadmap to guide you through the process:\n",
    "\n",
    "First we are going to download our necessary files:\n",
    "- Run the first cell to download the VCF (this may take a minute), when asked for the first VCF in our coding sections this will be the file we want to work with\n",
    "- From the GitHub repo, download these files from `Data-Processing-for-Genetics-QLS-MICM/Exercises/data`:\n",
    "  - hg19ToHg38.over.chain.gz\n",
    "  - Tutorial_Ancestry.txt\n",
    "  - Tutorial_Phenotype.txt\n",
    "\n",
    "### 🎯 **You're off to a great start!**\n",
    "Let's dive in!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown \"https://drive.google.com/uc?id=1iwJi7QfF3LElnCPen4fEQVoY4Vk4A9_5\"\n",
    "!tar -xzvf TUTORIAL.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import allel\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as PCA\n",
    "import pysam\n",
    "from pyliftover import LiftOver\n",
    "from typing import Dict, Generator, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chi2 as chi2_dist\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "#If you do not have a package here you can download via the command 'pip install ...' via your command line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_qc_table = []\n",
    "def count_variants(study_name, variant_data, sample_metadata, step_name):\n",
    "    # Count variants by chromosome\n",
    "    chroms = variant_data['variants/CHROM']\n",
    "    autosomal = sum((chrom.isdigit() and 1 <= int(chrom) <= 22) for chrom in chroms)\n",
    "    x_chr = sum(chrom in ['X', '23', '25'] for chrom in chroms)\n",
    "    y_chr = sum(chrom in ['Y', '24'] for chrom in chroms)\n",
    "    mt_chr = sum(chrom in ['MT', 'M', '26'] for chrom in chroms)\n",
    "    \n",
    "    # Count individuals by sex\n",
    "    sex_codes = sample_metadata['sex']\n",
    "    males = sum(sex == 1 for sex in sex_codes)\n",
    "    females = sum(sex == 2 for sex in sex_codes)\n",
    "    ambiguous = sum(sex == 0 for sex in sex_codes)\n",
    "    individuals = len(sex_codes)\n",
    "    \n",
    "    # Add to shared QC table\n",
    "    shared_qc_table.append([\n",
    "        study_name,\n",
    "        step_name,\n",
    "        autosomal,\n",
    "        x_chr,\n",
    "        y_chr,\n",
    "        mt_chr,\n",
    "        individuals,\n",
    "        males,\n",
    "        females,\n",
    "        ambiguous,\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"autosomal\": autosomal,\n",
    "        \"x_chr\": x_chr,\n",
    "        \"y_chr\": y_chr,\n",
    "        \"mt_chr\": mt_chr,\n",
    "        \"individuals\": individuals,\n",
    "        \"males\": males,\n",
    "        \"females\": females,\n",
    "        \"ambiguous\": ambiguous,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 🚀 LIFTOVER!  \n",
    "\n",
    "## What is a Genome Build?  \n",
    "A **genome build** is a **reference assembly** of a species' genome, providing a **standardized coordinate system** for genetic variants. Each build is an improved version of previous assemblies, correcting errors, adding missing sequences, and improving accuracy.  \n",
    "\n",
    "### Human Genome Builds:  \n",
    "- **hg18 (NCBI36)** – Released in **2006**  \n",
    "- **hg19 (GRCh37)** – Released in **2009**  \n",
    "- **hg38 (GRCh38)** – Released in **2013** (**Current Build**)  \n",
    "\n",
    "---\n",
    "\n",
    "## Why LiftOver?  \n",
    "You wouldn’t use an outdated **operating system** on your computer, right?  \n",
    "Similarly, we want to **update** our genomic data to the latest **genome build** to ensure accuracy.  \n",
    "\n",
    "This process is called **\"Lifting Over\"** because we **Liftover** our old data to the newest build.  \n",
    "\n",
    "### Failing to lift over can result in:  \n",
    "❌ **Mismatched variant positions**  \n",
    "❌ **Incorrect gene annotations**  \n",
    "❌ **Data incompatibility** with current research tools  \n",
    "\n",
    "---\n",
    "\n",
    "## Let's Gather Our Tools!  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg19/liftOver/hg19ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg18/liftOver/hg18ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/goldenPath/currentGenomes/Homo_sapiens/bigZips/hg38.fa.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load out VCFs through SciKit-allele to convert it into a format that works within python! \n",
    "**Do not unzip the chain file we have! Pyliftover is designed to handle the gunzipped version of the chain file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vcf(vcf_path: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load VCF file and return variant data and sample metadata\"\"\"\n",
    "    callset = allel.read_vcf(vcf_path)\n",
    "    \n",
    "    variant_data = {\n",
    "        'CHROM': callset['variants/CHROM'],\n",
    "        'POS': callset['variants/POS'],\n",
    "        'ID': callset['variants/ID'],\n",
    "        'REF': callset['variants/REF'],\n",
    "        'ALT': callset['variants/ALT'][:, 0],\n",
    "    }\n",
    "    \n",
    "    sample_metadata = {\n",
    "        'samples': callset['samples'],\n",
    "        'genotypes': callset['calldata/GT'],\n",
    "        'sex': callset.get('samples/sex', np.zeros(len(callset['samples']), dtype=int))\n",
    "    }\n",
    "    \n",
    "    return variant_data, sample_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will be a go to function we will be using to filter our data as we move along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variants(variant_data: Dict, sample_metadata: Dict, max_missing: float = 0.05) -> Tuple[Dict, Dict]:\n",
    "\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    missing_rate = gt.is_missing().mean(axis=1)\n",
    "    keep_mask = missing_rate <= max_missing\n",
    "    \n",
    "    # Filter both variant data and genotypes\n",
    "    filtered_variants = {\n",
    "        k: v[keep_mask] for k, v in variant_data.items()\n",
    "    }\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are about to liftoff our liftOver! We will be using pyliftover which is a python package adapted from the University of Califorina: Santa Cruz's original liftOver function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_liftover(variant_data: Dict, sample_metadata: Dict, chain_file: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    \n",
    "    CONCEPTUAL QUESTION: Why do we need to perform liftover?\n",
    "    FILL IN: We need liftover because different genome builds have _______ coordinate systems,\n",
    "    and we want to update our data to the _______ genome build to ensure _______ with\n",
    "    current research tools and databases.\n",
    "    \"\"\"\n",
    "    lo = LiftOver(chain_file)\n",
    "    n_variants = len(variant_data['CHROM'])\n",
    "    unmapped_mask = np.zeros(n_variants, dtype=bool)\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    new_chroms = variant_data['CHROM'].copy()\n",
    "    new_positions = variant_data['POS'].copy()\n",
    "    \n",
    "    for i, (chrom, pos) in enumerate(zip(variant_data['CHROM'], variant_data['POS'])):\n",
    "        chrom_clean = chrom.replace('chr', '')\n",
    "        \n",
    "        # FILL IN: VCF coordinates are 1-based, but liftover expects 0-based coordinates\n",
    "        # What adjustment do we need to make?\n",
    "        lifted = lo.convert_coordinate(f'chr{chrom_clean}', pos - ___)  # Convert to ___-based\n",
    "        \n",
    "        if not lifted:\n",
    "            unmapped_mask[i] = True\n",
    "        else:\n",
    "            # Update with lifted coordinates\n",
    "            lifted_result = lifted[0]\n",
    "            lifted_chrom = lifted_result[0]\n",
    "            lifted_pos = lifted_result[1]\n",
    "            new_chroms[i] = lifted_chrom\n",
    "            \n",
    "            # FILL IN: Convert back to the coordinate system used in VCF files\n",
    "            new_positions[i] = lifted_pos + ___  # Convert back to ___-based\n",
    "    \n",
    "    # Update variant_data with new coordinates\n",
    "    variant_data_lifted = variant_data.copy()\n",
    "    variant_data_lifted['CHROM'] = new_chroms\n",
    "    variant_data_lifted['POS'] = new_positions\n",
    "    \n",
    "    # Filter out unmapped variants\n",
    "    return filter_variants(variant_data_lifted, sample_metadata, ~unmapped_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have lift off! Now lets really get this car into gear and update our data to reflect our new mapped variants!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Ok! We have successfully performed a liftover! How do you feel? While the literal process of lifting over the coordinates has been completed, we are not quite done:\n",
    "    \n",
    "There’s still some housekeeping to take care of—after all, we didn’t generate this data ourselves, so we need to make sure everything’s in order. Here’s the game plan: \n",
    "\n",
    "---\n",
    "- **Map to Reference FASTA**: \n",
    "    - Let’s make sure those lifted-over variants actually match the sequence of the new genome build.\n",
    "- **Strand Flips**: \n",
    "    - Keep an eye out for major/minor (reference/alternate) allele mix-ups. Sometimes the reference strand orientation flips between builds, and we need to straighten that out.\n",
    "- **Palindromic SNPs**: \n",
    "    - These variants are indistinguishable on forward and reverse strands, making them a headache for allele assignment. Since they’re too ambiguous to resolve confidently, we’ll filter them out entirely\n",
    "- **Invalid SNPs**: \n",
    "    - Last but not least, filter out any variants where the reference allele doesn’t match the new build. Only the real deal makes the cut!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_reference(variant_data: Dict, sample_metadata: Dict, ref_fasta: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    \n",
    "    CONCEPTUAL QUESTIONS:\n",
    "    1. Why do we need to align to a reference genome after liftover?\n",
    "    2. What are palindromic SNPs and why are they problematic?\n",
    "    3. What does it mean when we \"flip\" alleles?\n",
    "    \"\"\"\n",
    "    ref = pysam.FastaFile(ref_fasta)\n",
    "    n_variants = len(variant_data['CHROM'])\n",
    "    \n",
    "    if sample_metadata['genotypes'].shape[0] != n_variants:\n",
    "        raise ValueError(f\"Dimension mismatch: {n_variants} variants vs {sample_metadata['genotypes'].shape[0]} genotypes\")\n",
    "    \n",
    "    remove_mask = np.zeros(n_variants, dtype=bool)\n",
    "    flip_mask = []\n",
    "    force_ref_mask = []\n",
    "    \n",
    "    for i, (chrom, pos, var_id, a1, a2) in enumerate(zip(\n",
    "        variant_data['CHROM'], variant_data['POS'], variant_data['ID'], \n",
    "        variant_data['REF'], variant_data['ALT']\n",
    "    )):\n",
    "        if chrom is None or pos is None:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Handle chromosome naming\n",
    "        chrom_fasta = chrom if chrom.startswith('chr') else f'chr{chrom}'\n",
    "        if chrom_fasta not in ref.references:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Get reference base\n",
    "        try:\n",
    "            ref_base = ref.fetch(chrom_fasta, pos - 1, pos).upper()\n",
    "        except:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # FILL IN: What are the four DNA bases we should keep?\n",
    "        valid_bases = {___, ___, ___, ___}\n",
    "        if a1 not in valid_bases or a2 not in valid_bases:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # FILL IN: Palindromic SNPs are problematic because they look the same on both strands\n",
    "        # What are the two types of palindromic SNP pairs?\n",
    "        palindromic_pairs = [{___, ___}, {___, ___}]\n",
    "        if {a1, a2} in palindromic_pairs:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Check allele alignment\n",
    "        if ref_base == a2:\n",
    "            force_ref_mask.append(i)\n",
    "        elif ref_base != a1:\n",
    "            # FILL IN: Create the complement mapping for strand flipping\n",
    "            # A pairs with T, C pairs with G\n",
    "            flip_map = {___: ___, ___: ___, ___: ___, ___: ___}\n",
    "            flipped_a1 = flip_map[a1]\n",
    "            flipped_a2 = flip_map[a2]\n",
    "            \n",
    "            if ref_base == flipped_a2:\n",
    "                force_ref_mask.append(i)\n",
    "                flip_mask.append(i)\n",
    "            elif ref_base == flipped_a1:\n",
    "                flip_mask.append(i)\n",
    "            else:\n",
    "                remove_mask[i] = True\n",
    "    \n",
    "    # Apply allele flips\n",
    "    # FILL IN: Create the complement mapping for strand flipping\n",
    "    # A pairs with T, C pairs with G\n",
    "    flip_map = {___: ___, ___: ___, ___: ___, ___: ___}\n",
    "    for i in flip_mask:\n",
    "        a1 = variant_data['REF'][i]\n",
    "        a2 = variant_data['ALT'][i]\n",
    "        variant_data['REF'][i] = flip_map[a1]\n",
    "        variant_data['ALT'][i] = flip_map[a2]\n",
    "    \n",
    "    # Force reference allele (swap REF and ALT)\n",
    "    for i in force_ref_mask:\n",
    "        ref_allele = variant_data['REF'][i]\n",
    "        alt_allele = variant_data['ALT'][i]\n",
    "        variant_data['REF'][i] = alt_allele\n",
    "        variant_data['ALT'][i] = ref_allele\n",
    "    \n",
    "    # Filter both variant data and genotypes\n",
    "    keep_mask = ~remove_mask\n",
    "    filtered_variants = {k: v[keep_mask] for k, v in variant_data.items()}\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets get to aligning! Be sure to remember where you sent the output of that lift over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can feed that to our alignment!\n",
    "\n",
    "---\n",
    "\n",
    "Be sure that your studies are being sent to **different directories** to ensure there is not issues with the alignment process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there was definetely some tidying to do, no matter how clean the data you inherited was!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We're Almost There! \n",
    "\n",
    "---\n",
    "\n",
    "###  **Great progress so far!** We've tackled the bulk of the challenging work, and now we’re down to the final steps to ensure our data is polished and ready for standard quality control. Let’s break it down:\n",
    "\n",
    "---\n",
    "\n",
    "##  **Handling Duplicate Variants**  \n",
    "Occasionally, multiple variants can appear at the same genomic position, which can complicate downstream analysis.  \n",
    "**Our goal:**  \n",
    "- Identify these duplicates and retain only the variant that is most prevalent in our dataset.   \n",
    "\n",
    "---\n",
    "\n",
    "##  **Standardizing Variant IDs**  \n",
    "Different studies often use different genotyping chips, each with its own naming conventions for variant IDs. This inconsistency can create headaches when integrating multiple datasets.  \n",
    "\n",
    "**Our solution:**  \n",
    "- Rename variant IDs using a universal format: **`chr:pos:REF:ALT`**.  \n",
    "- This ensures consistency across studies and makes our data more interoperable. Think of it as giving everyone the same map to follow! 🗺️  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_variants(variant_data: Dict) -> Dict:\n",
    "    \"\"\"Rename variants using chr:pos:ref:alt format\"\"\"\n",
    "    variant_data['ID'] = np.array([\n",
    "        f\"{chrom}:{pos}:{ref}:{alt}\"\n",
    "        for chrom, pos, ref, alt in zip(\n",
    "            variant_data['CHROM'], variant_data['POS'],\n",
    "            variant_data['REF'], variant_data['ALT']\n",
    "        )\n",
    "    ])\n",
    "    return variant_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have an updated and consistent dataset...time for analysis!!!\n",
    "\n",
    "---\n",
    "## **PCA**\n",
    "# We are going to conduct a Principal Component Analysis (PCA) reduces genetic data to reveal the main axes of variation, showing how populations cluster and diverge based on genetic similarity.\n",
    "\n",
    "---\n",
    "## **GWAS**\n",
    "# A Genome-Wide Association study (GWAS) is the bread and butter of genetic analysis, here we will be looking for variants statistically linked to traits or phenotypes tgat we may be interested in.\n",
    "\n",
    "# Before we can get started, there is some additional QC that we need to do to make sure that the data is sound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want to make sure that all the variants we look at are of relatively high genotyping in our dataset, so let us filter out variants that have exceptionally high missingness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness(variant_data: Dict, sample_metadata: Dict, max_missing_rate: float = 0.05) -> Tuple[Dict, Dict]:\n",
    "   \n",
    "    # Convert genotypes to scikit-allel GenotypeArray\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    \n",
    "    # Calculate missingness\n",
    "    missing = gt.is_missing()\n",
    "    missing_per_variant = np.sum(missing, axis=1)  # Sum missing calls per variant\n",
    "    total_samples = gt.n_samples\n",
    "    missing_rate = missing_per_variant / total_samples\n",
    "    \n",
    "    # Create filter mask\n",
    "    keep_mask = missing_rate <= max_missing_rate\n",
    "    \n",
    "    # Apply filtering to both variant data and genotypes\n",
    "    filtered_variant_data = {\n",
    "        'CHROM': variant_data['CHROM'][keep_mask],\n",
    "        'POS': variant_data['POS'][keep_mask],\n",
    "        'ID': variant_data['ID'][keep_mask],\n",
    "        'REF': variant_data['REF'][keep_mask],\n",
    "        'ALT': variant_data['ALT'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    filtered_sample_metadata = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variant_data, filtered_sample_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We’ll test if the variants in our dataset follow Hardy-Weinberg expectations. Deviations from HWE can indicate genotyping errors, population stratification, or other issues. It’s like a litmus test for data quality!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_hwe(variant_data: Dict, sample_metadata: Dict, p_threshold: float = 1e-25) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    \n",
    "    CONCEPTUAL QUESTIONS:\n",
    "    1. What does Hardy-Weinberg Equilibrium assume about a population?\n",
    "    2. What might cause deviations from HWE?\n",
    "    3. Why might we want to filter out variants that strongly deviate from HWE?\n",
    "    \n",
    "    FILL IN: HWE assumes random _______, no _______, no _______, and no _______.\n",
    "    Deviations might indicate _______ errors, population _______, or _______ selection.\n",
    "    \"\"\"\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    n_variants = gt.shape[0]\n",
    "    keep_mask = np.ones(n_variants, dtype=bool)\n",
    "    \n",
    "    for i in range(n_variants):\n",
    "        # Get genotypes for this variant\n",
    "        variant_genotypes = gt[i]\n",
    "        \n",
    "        # FILL IN: Count the three possible genotype classes\n",
    "        # Homozygous reference (0,0), heterozygous (0,1 or 1,0), homozygous alternate (1,1)\n",
    "        n_AA = np.sum((variant_genotypes == [___, ___]).all(axis=1))  # Homozygous reference\n",
    "        n_Aa = np.sum(((variant_genotypes == [___, ___]).all(axis=1)) | \n",
    "                      ((variant_genotypes == [___, ___]).all(axis=1)))  # Heterozygous\n",
    "        n_aa = np.sum((variant_genotypes == [___, ___]).all(axis=1))  # Homozygous alternate\n",
    "        \n",
    "        n = n_AA + n_Aa + n_aa\n",
    "        \n",
    "        if n == 0:\n",
    "            keep_mask[i] = False\n",
    "            continue\n",
    "            \n",
    "        # FILL IN: Calculate allele frequencies\n",
    "        # p = frequency of reference allele, q = frequency of alternate allele\n",
    "        p = (2*n_AA + n_Aa) / (___*n)  # Why do we multiply n_AA by 2?\n",
    "        q = ___ - p  # What should q equal?\n",
    "        \n",
    "        # Skip if monomorphic (p or q is 0)\n",
    "        if p == 0 or q == 0:\n",
    "            keep_mask[i] = False\n",
    "            continue\n",
    "            \n",
    "        # FILL IN: Calculate expected genotype frequencies under HWE\n",
    "        # What are the expected frequencies for each genotype class?\n",
    "        exp_AA = ___*___ * n  # p squared times n\n",
    "        exp_Aa = ___*___*___ * n  # 2pq times n\n",
    "        exp_aa = ___*___ * n  # q squared times n\n",
    "        \n",
    "        # Chi-square test \n",
    "        chi2 = 0\n",
    "        if exp_AA > 0:\n",
    "            chi2 += (n_AA - exp_AA)**2 / exp_AA\n",
    "        if exp_Aa > 0:\n",
    "            chi2 += (n_Aa - exp_Aa)**2 / exp_Aa\n",
    "        if exp_aa > 0:\n",
    "            chi2 += (n_aa - exp_aa)**2 / exp_aa\n",
    "            \n",
    "        # Convert to p-value\n",
    "        pval = chi2_dist.sf(chi2, df=1)\n",
    "        keep_mask[i] = pval >= p_threshold\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_variants = {k: v[keep_mask] for k, v in variant_data.items()}\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for minor allele frequency (MAF) trims out rare variants that are more likely to be sequencing noise or uninformative for analysis—like clearing static from a radio signal to hear the true genetic tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_maf(variant_data: Dict, sample_metadata: Dict, maf_threshold: float = 0.01) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    \n",
    "    CONCEPTUAL QUESTION: Why do we filter out variants with very low MAF?\n",
    "    FILL IN: Variants with very low MAF have limited _______ power in association studies,\n",
    "    are more likely to be _______ errors, and may not be _______ of the broader population.\n",
    "    \"\"\"\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    allele_counts = gt.count_alleles()\n",
    "    \n",
    "    # Calculate MAF for each variant\n",
    "    maf = np.min(allele_counts[:, :2] / np.sum(allele_counts[:, :2], axis=1)[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # Create filter mask\n",
    "    keep_mask = (maf >= maf_threshold) & ~np.isnan(maf)\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_variants = {k: v[keep_mask] for k, v in variant_data.items()}\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(variant_data: Dict, sample_metadata: Dict, population_file: str,, n_components: int = 10):\n",
    "    \"\"\"\n",
    "    \n",
    "    CONCEPTUAL QUESTIONS:\n",
    "    1. What does PCA do to genetic data?\n",
    "    2. Why might we see population clusters in a PCA plot?\n",
    "    3. Why do we standardize the data before PCA?\n",
    "    \n",
    "    FILL IN: PCA reduces the _______ of genetic data while preserving the main\n",
    "    axes of _______. It helps us visualize population _______ and identify\n",
    "    genetic _______ between individuals.\n",
    "    \"\"\"\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    pop_df = pd.read_csv(population_file, sep='\\t', header=True)\n",
    "    pop_dict = dict(zip(pop_df['sample_id'], pop_df['pop']))\n",
    "\n",
    "    # FILL IN: Convert genotypes to allele counts and subsample\n",
    "    # Why might we take every 5th variant instead of using all variants?\n",
    "    gn = gt.to_n_alt()[::___]  # Take every ___th variant for efficiency\n",
    "    \n",
    "    # Transpose to get samples as rows\n",
    "    gn_t = gn.T\n",
    "    \n",
    "    # FILL IN: Why do we standardize the data?\n",
    "    # Standardization ensures all variants contribute _______ to the analysis\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    gn_std = scaler.fit_transform(gn_t)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(gn_std)\n",
    "    sample_ids = sample_metadata['samples']\n",
    "    populations = [pop_dict.get(sample_id, 'Unknown') for sample_id in sample_ids]\n",
    "    unique_pops = list(set(populations))\n",
    "    colors = [\"#4363d8\",\"#e6194b\",\"#3cb44b\",\"#f58231\",\"#911eb4\",\"#33b2b2\",\"#af2ca8\",\"#96c603\",\"#ca5d5d\",\"#008080\", \"#654977\",\"#29699E\",\"#8f862d\",\"#800000\",\"#4f9c66\",\"#808000\",\"#6e75b7\",\"#000075\",\"#662245\",\"#ff0000\", \"#DCD42E\", \"#44af69\", \"#f8333c\", \"#fcab10\", \"#2b9eb3\", \"#dbd5b5\", \"#ffdd19\"]\n",
    "    color_map = dict(zip(unique_pops, colors))\n",
    "\n",
    "    # Plot first two PCs\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for pop in unique_pops:\n",
    "        mask = [p == pop for p in populations]\n",
    "        plt.scatter(pcs[mask, 0], pcs[mask, 1], \n",
    "                   c=[color_map[pop]], \n",
    "                   label=pop, \n",
    "                   alpha=0.7, \n",
    "                   s=50)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now we can finally run the PCA! Keep your eye out for distinctive clusters! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can do the GWAS, we will be visuallizing the results of our GWAS in a plot known as a Manhattan Plot; A Manhattan plot shows GWAS results as a skyline of p-values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_linregress(genotypes, phenotypes):\n",
    "    p_values = []\n",
    "    for SNP_index in range(genotypes.shape[0]): \n",
    "        x= genotypes[SNP_index, :]\n",
    "        results= stats.linregress(x,phenotypes)\n",
    "        p_values.append(results.pvalue) \n",
    "    \n",
    "    return p_values\n",
    "def run_gwas(variant_data: Dict, sample_metadata: Dict, phenotype_file: str):\n",
    "    \"\"\"\n",
    "    Run Genome-Wide Association Study (GWAS)\n",
    "    \n",
    "    CONCEPTUAL QUESTIONS:\n",
    "    1. What is a GWAS trying to find?\n",
    "    2. What does the red line in a Manhattan plot represent?\n",
    "    3. Why do we use -log10(p-value) on the y-axis?\n",
    "    \n",
    "    FILL IN: GWAS looks for genetic variants that are _______ associated with\n",
    "    a trait or disease. The red line represents the threshold for _______-wide\n",
    "    significance (typically ___ × 10^-8).\n",
    "    \"\"\"\n",
    "    # Load phenotype data \n",
    "    pheno_df = pd.read_csv(phenotype_file,  sep='\\t',  header=None, names=['sample', 'phenotype'])\n",
    "    pheno_df = pheno_df.set_index('IID')\n",
    "    \n",
    "    # Align genotypes with phenotypes\n",
    "    sample_ids = sample_metadata['samples']\n",
    "    phenotypes = pheno_df.loc[sample_ids]['phenotype'].values\n",
    "    \n",
    "    # Convert genotypes to allele counts\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    gn = gt.to_n_alt()  # shape: (n_variants, n_samples)\n",
    "    \n",
    "    # Run regression\n",
    "    p_values = association_linregress(gn, phenotypes)\n",
    "    \n",
    "    # Create Manhattan plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    chrom = variant_data['CHROM']\n",
    "    pos = variant_data['POS']\n",
    "    \n",
    "    # Convert chromosome names to numeric\n",
    "    unique_chroms = set(chrom)\n",
    "    def parse_chromosome_name(chrom_name):\n",
    "        chrom_part = chrom_name.replace('chr', '')  # Remove 'chr' prefix\n",
    "        main_part = chrom_part.split('_')[0]       # Some datasets with annotate their chromosomes with underscores!\n",
    "    \n",
    "        if main_part.isdigit():\n",
    "            return int(main_part)   \n",
    "        elif main_part == 'X':\n",
    "            return 23               \n",
    "        elif main_part == 'Y':\n",
    "            return 24  \n",
    "        elif main_part == 'MT' or main_part == 'M':\n",
    "            return 25  \n",
    "        else:\n",
    "            return 0 \n",
    "    chrom_num = [parse_chromosome_name(c) for c in chrom]\n",
    "    \n",
    "    # Create plotting dataframe\n",
    "    gwas_df = pd.DataFrame({'CHR': chrom_num, 'BP': pos, 'P': p_values})\n",
    "    \n",
    "    # Manhattan plot \n",
    "    gwas_df = gwas_df.sort_values(['CHR', 'BP'])\n",
    "    gwas_df['ind'] = range(len(gwas_df))\n",
    "    gwas_df_grouped = gwas_df.groupby('CHR')\n",
    "    \n",
    "    colors = ['#fc0f0f', '#fc9a0f', '#fcf80f', '#4efc0f', '#0fa5fc', '#c10ffc', '#fc0fb5']  \n",
    "    x_labels = []\n",
    "    x_labels_pos = []\n",
    "    \n",
    "    for num, (name, group) in enumerate(gwas_df_grouped):\n",
    "        color = colors[num % len(colors)]\n",
    "        plt.scatter(group['ind'], -np.log10(group['P']), color=color, s=2)\n",
    "        x_labels.append(name)\n",
    "        x_labels_pos.append((group['ind'].iloc[-1] - (group['ind'].iloc[-1] - group['ind'].iloc[0])/2))\n",
    "    \n",
    "    # FILL IN: What p-value threshold represents genome-wide significance?\n",
    "    plt.axhline(y=-np.log10(___e-___), color='r', linestyle='--', label='Genome-wide significance')\n",
    "    plt.xticks(x_labels_pos, x_labels)\n",
    "    plt.xlabel('Chromosome')\n",
    "    plt.ylabel('-log10(p-value)')\n",
    "    plt.title('Manhattan Plot - GWAS Results')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return p_values, gwas_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you guess what the phenotype was based on the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can save our work! This will likely take several minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_vcf(variant_data: dict, sample_metadata: dict, output_path: str):\n",
    "    samples = sample_metadata['samples']\n",
    "    genotypes = sample_metadata['genotypes']  # shape: (num_variants, num_samples, 2)\n",
    "\n",
    "    with gzip.open(output_path, 'wt') as f:\n",
    "        # Write VCF header\n",
    "        f.write(\"##fileformat=VCFv4.2\\n\")\n",
    "        f.write(\"##source=Generated by Tutorial\\n\")\n",
    "        for chrom in sorted(set(variant_data['CHROM'])):\n",
    "            chrom_str = f\"chr{chrom}\" if not str(chrom).startswith(\"chr\") else str(chrom)\n",
    "            f.write(f\"##contig=<ID={chrom_str}>\\n\")\n",
    "        f.write(\"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\")\n",
    "        f.write(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\t\")\n",
    "        f.write(\"\\t\".join(samples) + \"\\n\")\n",
    "\n",
    "        # Write variants\n",
    "        num_variants = len(variant_data['CHROM'])\n",
    "        for i in range(num_variants):\n",
    "            chrom = variant_data['CHROM'][i]\n",
    "            chrom_str = f\"chr{chrom}\" if not str(chrom).startswith(\"chr\") else str(chrom)\n",
    "            pos = variant_data['POS'][i]\n",
    "            vid = variant_data['ID'][i]\n",
    "            ref = variant_data['REF'][i]\n",
    "            alt = variant_data['ALT'][i] if variant_data['ALT'][i] not in ('.', None) else '.'\n",
    "            qual = str(variant_data['QUAL'][i]) if 'QUAL' in variant_data else '.'\n",
    "            filt = variant_data['FILTER'][i] if 'FILTER' in variant_data else '.'\n",
    "            if filt in (None, '.'):\n",
    "                filt = '.'\n",
    "            info = '.'\n",
    "            fmt = 'GT'\n",
    "\n",
    "            # Process genotypes as strings\n",
    "            gt_strings = []\n",
    "            for j in range(len(samples)):\n",
    "                gt = genotypes[i, j]\n",
    "                if gt[0] == -1 or gt[1] == -1:\n",
    "                    gt_str = './.'\n",
    "                else:\n",
    "                    gt_str = f\"{int(gt[0])}/{int(gt[1])}\"\n",
    "                gt_strings.append(gt_str)\n",
    "\n",
    "            line = f\"{chrom_str}\\t{pos}\\t{vid}\\t{ref}\\t{alt}\\t{qual}\\t{filt}\\t{info}\\t{fmt}\\t\" + \"\\t\".join(gt_strings)\n",
    "            f.write(line + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKSHOP WORKFLOW EXAMPLE:\n",
    "\"\"\"\n",
    "# Step 1: Load your VCF file\n",
    "variant_data, sample_metadata = load_vcf(\"file.vcf.gz\")\n",
    "\n",
    "# Step 2: Perform liftover \n",
    "variant_data, sample_metadata = perform_liftover(variant_data, sample_metadata, \"hg19ToHg38.over.chain.gz)\n",
    "\n",
    "# Step 3: Align to reference genome\n",
    "variant_data, sample_metadata = align_to_reference(variant_data, sample_metadata, \"hg38.fa\")\n",
    "\n",
    "# Step 4: Rename variants for consistency\n",
    "variant_data = rename_variants(variant_data)\n",
    "\n",
    "# Step 5: Quality control filters\n",
    "variant_data, sample_metadata = missingness(variant_data, sample_metadata)\n",
    "variant_data, sample_metadata = filter_by_hwe(variant_data, sample_metadata)\n",
    "variant_data, sample_metadata = filter_by_maf(variant_data, sample_metadata)\n",
    "\n",
    "# Step 6: Run PCA\n",
    "pcs = run_pca(variant_data, sample_metadata. \"ancestry.txt\")\n",
    "\n",
    "# Step 7: Run GWAS (if you have phenotype data)\n",
    "p_values, gwas_results = run_gwas(variant_data, sample_metadata, \"phenotypes.txt\")\n",
    "\n",
    "# Step 8: Save results\n",
    "save_vcf(variant_data, sample_metadata, \"processed.vcf.gz\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 **You Did It! Look at You Go, Hot Shot!** 🎉\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benjie's Fun time Jupyter Hub",
   "language": "python",
   "name": "Benjie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
