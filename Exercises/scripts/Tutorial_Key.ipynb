{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ðŸŽ‰ Welcome to the basics of working with genetic data! ðŸŽ‰\n",
    "\n",
    "---\n",
    "\n",
    "###  **Letâ€™s get started!** We will be going over some of the foundataional quality control measures and analyses in population genetics. Hereâ€™s a roadmap to guide you through the process:\n",
    "\n",
    "\n",
    "### ðŸŽ¯ **Youâ€™re off to a great start!**  \n",
    "Letâ€™s dive in!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1iwJi7QfF3LElnCPen4fEQVoY4Vk4A9_5\n",
    "!tar -xzvf TUTORIAL.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import allel\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as PCA\n",
    "import pysam\n",
    "from pyliftover import LiftOver\n",
    "from typing import Dict, Generator, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chi2 as chi2_dist\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "#If you do not have a package here you can download via the command 'pip install ...' via your command line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_qc_table = []\n",
    "def count_variants(study_name, variant_data, sample_metadata, step_name):\n",
    "    # Count variants by chromosome\n",
    "    chroms = variant_data['variants/CHROM']\n",
    "    autosomal = sum((chrom.isdigit() and 1 <= int(chrom) <= 22) for chrom in chroms)\n",
    "    x_chr = sum(chrom in ['X', '23', '25'] for chrom in chroms)\n",
    "    y_chr = sum(chrom in ['Y', '24'] for chrom in chroms)\n",
    "    mt_chr = sum(chrom in ['MT', 'M', '26'] for chrom in chroms)\n",
    "    \n",
    "    # Count individuals by sex\n",
    "    sex_codes = sample_metadata['sex']\n",
    "    males = sum(sex == 1 for sex in sex_codes)\n",
    "    females = sum(sex == 2 for sex in sex_codes)\n",
    "    ambiguous = sum(sex == 0 for sex in sex_codes)\n",
    "    individuals = len(sex_codes)\n",
    "    \n",
    "    # Add to shared QC table\n",
    "    shared_qc_table.append([\n",
    "        study_name,\n",
    "        step_name,\n",
    "        autosomal,\n",
    "        x_chr,\n",
    "        y_chr,\n",
    "        mt_chr,\n",
    "        individuals,\n",
    "        males,\n",
    "        females,\n",
    "        ambiguous,\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"autosomal\": autosomal,\n",
    "        \"x_chr\": x_chr,\n",
    "        \"y_chr\": y_chr,\n",
    "        \"mt_chr\": mt_chr,\n",
    "        \"individuals\": individuals,\n",
    "        \"males\": males,\n",
    "        \"females\": females,\n",
    "        \"ambiguous\": ambiguous,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ðŸš€ LIFTOVER!  \n",
    "\n",
    "## What is a Genome Build?  \n",
    "A **genome build** is a **reference assembly** of a species' genome, providing a **standardized coordinate system** for genetic variants. Each build is an improved version of previous assemblies, correcting errors, adding missing sequences, and improving accuracy.  \n",
    "\n",
    "### Human Genome Builds:  \n",
    "- **hg18 (NCBI36)** â€“ Released in **2006**  \n",
    "- **hg19 (GRCh37)** â€“ Released in **2009**  \n",
    "- **hg38 (GRCh38)** â€“ Released in **2013** (**Current Build**)  \n",
    "\n",
    "---\n",
    "\n",
    "## Why LiftOver?  \n",
    "You wouldnâ€™t use an outdated **operating system** on your computer, right?  \n",
    "Similarly, we want to **update** our genomic data to the latest **genome build** to ensure accuracy.  \n",
    "\n",
    "This process is called **\"Lifting Over\"** because we **Liftover** our old data to the newest build.  \n",
    "\n",
    "### Failing to lift over can result in:  \n",
    "âŒ **Mismatched variant positions**  \n",
    "âŒ **Incorrect gene annotations**  \n",
    "âŒ **Data incompatibility** with current research tools  \n",
    "\n",
    "---\n",
    "\n",
    "## Let's Gather Our Tools!  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg19/liftOver/hg19ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg18/liftOver/hg18ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/goldenPath/currentGenomes/Homo_sapiens/bigZips/hg38.fa.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load out VCFs through SciKit-allele to convert it into a format that works within python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vcf(vcf_path: str) -> Tuple[Dict, Dict]:\n",
    "    callset = allel.read_vcf(vcf_path)\n",
    "    \n",
    "    variant_data = {\n",
    "        'CHROM': callset['variants/CHROM'],\n",
    "        'POS': callset['variants/POS'],\n",
    "        'ID': callset['variants/ID'],\n",
    "        'REF': callset['variants/REF'],\n",
    "        'ALT': callset['variants/ALT'][:, 0],\n",
    "    }\n",
    "    \n",
    "    sample_metadata = {\n",
    "        'samples': callset['samples'],\n",
    "        'genotypes': callset['calldata/GT'],\n",
    "        'sex': callset.get('samples/sex', np.zeros(len(callset['samples']), dtype=int))\n",
    "    }\n",
    "    \n",
    "    return variant_data, sample_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data, sample_metadata = load_vcf(\"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will be a go to function we will be using to filter our data as we move along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variants(variant_data: Dict, sample_metadata: Dict, max_missing: float = 0.05) -> Tuple[Dict, Dict]:\n",
    "\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    missing_rate = gt.is_missing().mean(axis=1)\n",
    "    keep_mask = missing_rate <= max_missing\n",
    "    \n",
    "    # Filter both variant data and genotypes\n",
    "    filtered_variants = {\n",
    "        k: v[keep_mask] for k, v in variant_data.items()\n",
    "    }\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are about to liftoff our liftOver! We will be using pyliftover which is a python package adapted from the University of Califorina: Santa Cruz's original liftOver function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_liftover(variant_data: Dict, sample_metadata: Dict, chain_file: str) -> Tuple[Dict, Dict]:\n",
    "    lo = LiftOver(chain_file)\n",
    "    n_variants = len(variant_data['CHROM'])\n",
    "    unmapped_mask = np.zeros(n_variants, dtype=bool)\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    new_chroms = variant_data['CHROM'].copy()\n",
    "    new_positions = variant_data['POS'].copy()\n",
    "    \n",
    "    for i, (chrom, pos) in enumerate(zip(variant_data['CHROM'], variant_data['POS'])):\n",
    "        chrom_clean = chrom.replace('chr', '')\n",
    "        lifted = lo.convert_coordinate(f'chr{chrom_clean}', pos - 1)  # Convert to 0-based\n",
    "        \n",
    "        if not lifted:\n",
    "            unmapped_mask[i] = True\n",
    "        else:\n",
    "            # Update with lifted coordinates\n",
    "            lifted_result = lifted[0]\n",
    "            lifted_chrom = lifted_result[0]\n",
    "            lifted_pos = lifted_result[1]\n",
    "            new_chroms[i] = lifted_chrom\n",
    "            new_positions[i] = lifted_pos + 1  # Convert back to 1-based\n",
    "    \n",
    "    # Update variant_data with new coordinates\n",
    "    variant_data_lifted = variant_data.copy()\n",
    "    variant_data_lifted['CHROM'] = new_chroms\n",
    "    variant_data_lifted['POS'] = new_positions\n",
    "    \n",
    "    # Filter out unmapped variants\n",
    "    return filter_variants(variant_data_lifted, sample_metadata, ~unmapped_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data, sample_metadata = perform_liftover(variant_data, sample_metadata, \"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have lift off! Now lets really get this car into gear and update our data to reflect our new mapped variants!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Ok! We have successfully performed a liftover! How do you feel? While the literal process of lifting over the coordinates has been completed, we are not quite done:\n",
    "    \n",
    "Thereâ€™s still some housekeeping to take care ofâ€”after all, we didnâ€™t generate this data ourselves, so we need to make sure everythingâ€™s in order. Hereâ€™s the game plan: \n",
    "\n",
    "---\n",
    "- **Map to Reference FASTA**: \n",
    "    - Letâ€™s make sure those lifted-over variants actually match the sequence of the new genome build.\n",
    "- **Strand Flips**: \n",
    "    - Keep an eye out for major/minor (reference/alternate) allele mix-ups. Sometimes the reference strand orientation flips between builds, and we need to straighten that out.\n",
    "- **Palindromic SNPs**: \n",
    "    - These variants are indistinguishable on forward and reverse strands, making them a headache for allele assignment. Since theyâ€™re too ambiguous to resolve confidently, weâ€™ll filter them out entirely\n",
    "- **Invalid SNPs**: \n",
    "    - Last but not least, filter out any variants where the reference allele doesnâ€™t match the new build. Only the real deal makes the cut!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_reference(variant_data: Dict, sample_metadata: Dict, ref_fasta: str) -> Tuple[Dict, Dict]:\n",
    "    ref = pysam.FastaFile(ref_fasta)\n",
    "    n_variants = len(variant_data['CHROM'])\n",
    "    \n",
    "    # Validate dimensions match\n",
    "    if sample_metadata['genotypes'].shape[0] != n_variants:\n",
    "        raise ValueError(f\"Dimension mismatch: {n_variants} variants vs {sample_metadata['genotypes'].shape[0]} genotypes\")\n",
    "    \n",
    "    remove_mask = np.zeros(n_variants, dtype=bool)\n",
    "    flip_mask = []\n",
    "    force_ref_mask = []\n",
    "    \n",
    "    for i, (chrom, pos, var_id, a1, a2) in enumerate(zip(\n",
    "        variant_data['CHROM'],\n",
    "        variant_data['POS'],\n",
    "        variant_data['ID'],\n",
    "        variant_data['REF'],\n",
    "        variant_data['ALT']\n",
    "    )):\n",
    "        # Skip if variant wasn't mapped\n",
    "        if chrom is None or pos is None:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Handle chromosome naming\n",
    "        chrom_fasta = chrom if chrom.startswith('chr') else f'chr{chrom}'\n",
    "        if chrom_fasta not in ref.references:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Get reference base\n",
    "        try:\n",
    "            ref_base = ref.fetch(chrom_fasta, pos - 1, pos).upper()\n",
    "        except:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Skip non-SNPs\n",
    "        if a1 not in {'A', 'C', 'G', 'T'} or a2 not in {'A', 'C', 'G', 'T'}:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Skip palindromic SNPs\n",
    "        if {a1, a2} in [{'A', 'T'}, {'C', 'G'}]:\n",
    "            remove_mask[i] = True\n",
    "            continue\n",
    "            \n",
    "        # Check allele alignment\n",
    "        if ref_base == a2:\n",
    "            force_ref_mask.append(i)\n",
    "        elif ref_base != a1:\n",
    "            # Check if flipped alleles match\n",
    "            flip_map = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "            flipped_a1 = flip_map[a1]\n",
    "            flipped_a2 = flip_map[a2]\n",
    "            \n",
    "            if ref_base == flipped_a2:\n",
    "                force_ref_mask.append(i)\n",
    "                flip_mask.append(i)\n",
    "            elif ref_base == flipped_a1:\n",
    "                flip_mask.append(i)\n",
    "            else:\n",
    "                remove_mask[i] = True\n",
    "    \n",
    "    # Apply allele flips\n",
    "    for i in flip_mask:\n",
    "        a1 = variant_data['REF'][i]\n",
    "        a2 = variant_data['ALT'][i]\n",
    "        variant_data['REF'][i] = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}[a1]\n",
    "        variant_data['ALT'][i] = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}[a2]\n",
    "    \n",
    "    # Force reference allele\n",
    "    for i in force_ref_mask:\n",
    "        ref_allele = variant_data['REF'][i]\n",
    "        alt_allele = variant_data['ALT'][i]\n",
    "        variant_data['REF'][i] = alt_allele\n",
    "        variant_data['ALT'][i] = ref_allele\n",
    "    \n",
    "    # Filter both variant data and genotypes\n",
    "    keep_mask = ~remove_mask\n",
    "    filtered_variants = {\n",
    "        'CHROM': variant_data['CHROM'][keep_mask],\n",
    "        'POS': variant_data['POS'][keep_mask],\n",
    "        'ID': variant_data['ID'][keep_mask],\n",
    "        'REF': variant_data['REF'][keep_mask],\n",
    "        'ALT': variant_data['ALT'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data, sample_metadata = align_to_reference(variant_data,  sample_metadata,\"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets get to aligning! Be sure to remember where you sent the output of that lift over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can feed that to our alignment!\n",
    "\n",
    "---\n",
    "\n",
    "Be sure that your studies are being sent to **different directories** to ensure there is not issues with the alignment process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there was definetely some tidying to do, no matter how clean the data you inherited was!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We're Almost There! \n",
    "\n",
    "---\n",
    "\n",
    "###  **Great progress so far!** We've tackled the bulk of the challenging work, and now weâ€™re down to the final steps to ensure our data is polished and ready for standard quality control. Letâ€™s break it down:\n",
    "\n",
    "---\n",
    "\n",
    "##  **Handling Duplicate Variants**  \n",
    "Occasionally, multiple variants can appear at the same genomic position, which can complicate downstream analysis.  \n",
    "**Our goal:**  \n",
    "- Identify these duplicates and retain only the variant that is most prevalent in our dataset.   \n",
    "\n",
    "---\n",
    "\n",
    "##  **Standardizing Variant IDs**  \n",
    "Different studies often use different genotyping chips, each with its own naming conventions for variant IDs. This inconsistency can create headaches when integrating multiple datasets.  \n",
    "\n",
    "**Our solution:**  \n",
    "- Rename variant IDs using a universal format: **`chr:pos:REF:ALT`**.  \n",
    "- This ensures consistency across studies and makes our data more interoperable. Think of it as giving everyone the same map to follow! ðŸ—ºï¸  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_variants(variant_data: Dict) -> Dict:\n",
    "    \"\"\"Rename variants using chr:pos:ref:alt format\"\"\"\n",
    "    variant_data['ID'] = np.array([\n",
    "        f\"{chrom}:{pos}:{ref}:{alt}\"\n",
    "        for chrom, pos, ref, alt in zip(\n",
    "            variant_data['CHROM'], variant_data['POS'],\n",
    "            variant_data['REF'], variant_data['ALT']\n",
    "        )\n",
    "    ])\n",
    "    return variant_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data = rename_variants(variant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have an updated and consistent dataset...time for analysis!!!\n",
    "\n",
    "---\n",
    "## **PCA**\n",
    "# We are going to conduct a Principal Component Analysis (PCA) reduces genetic data to reveal the main axes of variation, showing how populations cluster and diverge based on genetic similarity.\n",
    "\n",
    "---\n",
    "## **GWAS**\n",
    "# A Genome-Wide Association study (GWAS) is the bread and butter of genetic analysis, here we will be looking for variants statistically linked to traits or phenotypes tgat we may be interested in.\n",
    "\n",
    "# Before we can get started, there is some additional QC that we need to do to make sure that the data is sound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want to make sure that all the variants we look at are of relatively high genotyping in our dataset, so let us filter out variants that have exceptionally high missingness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness(variant_data: Dict, sample_metadata: Dict, max_missing_rate: float = 0.05) -> Tuple[Dict, Dict]:\n",
    "   \n",
    "    # Convert genotypes to scikit-allel GenotypeArray\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    \n",
    "    # Calculate missingness\n",
    "    missing = gt.is_missing()\n",
    "    missing_per_variant = np.sum(missing, axis=1)  # Sum missing calls per variant\n",
    "    total_samples = gt.n_samples\n",
    "    missing_rate = missing_per_variant / total_samples\n",
    "    \n",
    "    # Create filter mask\n",
    "    keep_mask = missing_rate <= max_missing_rate\n",
    "    \n",
    "    # Apply filtering to both variant data and genotypes\n",
    "    filtered_variant_data = {\n",
    "        'CHROM': variant_data['CHROM'][keep_mask],\n",
    "        'POS': variant_data['POS'][keep_mask],\n",
    "        'ID': variant_data['ID'][keep_mask],\n",
    "        'REF': variant_data['REF'][keep_mask],\n",
    "        'ALT': variant_data['ALT'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    filtered_sample_metadata = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variant_data, filtered_sample_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_variants, filtered_samples = missingness(variant_data,sample_metadata,max_missing_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weâ€™ll test if the variants in our dataset follow Hardy-Weinberg expectations. Deviations from HWE can indicate genotyping errors, population stratification, or other issues. Itâ€™s like a litmus test for data quality!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_hwe(variant_data: Dict,sample_metadata: Dict, p_threshold: float = 1e-25) -> Tuple[Dict, Dict]:\n",
    "\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    n_variants = gt.shape[0]\n",
    "    keep_mask = np.ones(n_variants, dtype=bool)\n",
    "    \n",
    "    for i in range(n_variants):\n",
    "        # Get genotypes for this variant\n",
    "        variant_genotypes = gt[i]\n",
    "        \n",
    "        # Count genotypes manually\n",
    "        \n",
    "        n_AA = np.sum((variant_genotypes == [0, 0]).all(axis=1))  # Homozygous reference\n",
    "        n_Aa = np.sum(((variant_genotypes == [0, 1]).all(axis=1)) | \n",
    "                      ((variant_genotypes == [1, 0]).all(axis=1)))  # Heterozygous\n",
    "        n_aa = np.sum((variant_genotypes == [1, 1]).all(axis=1))  # Homozygous alternate\n",
    "        \n",
    "        n = n_AA + n_Aa + n_aa\n",
    "        \n",
    "        if n == 0:\n",
    "            keep_mask[i] = False\n",
    "            continue\n",
    "            \n",
    "        # Calculate allele frequencies\n",
    "        p = (2*n_AA + n_Aa) / (2*n)\n",
    "        q = 1 - p\n",
    "        \n",
    "        # Skip if p or q is 0 (monomorphic)\n",
    "        if p == 0 or q == 0:\n",
    "            keep_mask[i] = False\n",
    "            continue\n",
    "            \n",
    "        # Expected genotype frequencies\n",
    "        exp_AA = p*p * n\n",
    "        exp_Aa = 2*p*q * n\n",
    "        exp_aa = q*q * n\n",
    "        \n",
    "        # Chi-square test \n",
    "        chi2 = 0\n",
    "        if exp_AA > 0:\n",
    "            chi2 += (n_AA - exp_AA)**2 / exp_AA\n",
    "        if exp_Aa > 0:\n",
    "            chi2 += (n_Aa - exp_Aa)**2 / exp_Aa\n",
    "        if exp_aa > 0:\n",
    "            chi2 += (n_aa - exp_aa)**2 / exp_aa\n",
    "            \n",
    "        # Convert to p-value\n",
    "        \n",
    "        pval = chi2_dist.sf(chi2, df=1)\n",
    "        \n",
    "        keep_mask[i] = pval >= p_threshold\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_variants = {k: v[keep_mask] for k, v in variant_data.items()}\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data, sample_metadata = filter_by_hwe(variant_data, sample_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for minor allele frequency (MAF) trims out rare variants that are more likely to be sequencing noise or uninformative for analysisâ€”like clearing static from a radio signal to hear the true genetic tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_maf(variant_data: Dict, sample_metadata: Dict, maf_threshold: float = 0.01) -> Tuple[Dict, Dict]:\n",
    "\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    allele_counts = gt.count_alleles()\n",
    "    \n",
    "    # Calculate MAF for each variant\n",
    "    maf = np.min(allele_counts[:, :2] / np.sum(allele_counts[:, :2], axis=1)[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # Create filter mask\n",
    "    keep_mask = (maf >= maf_threshold) & ~np.isnan(maf)\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_variants = {\n",
    "        k: v[keep_mask] for k, v in variant_data.items()\n",
    "    }\n",
    "    filtered_samples = {\n",
    "        'samples': sample_metadata['samples'],\n",
    "        'sex': sample_metadata['sex'],\n",
    "        'genotypes': sample_metadata['genotypes'][keep_mask]\n",
    "    }\n",
    "    \n",
    "    return filtered_variants, filtered_samples\n",
    "\n",
    "\n",
    "\n",
    "def run_pca(variant_data: Dict, sample_metadata: Dict, population_file: str, n_components: int = 10):\n",
    "\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    pop_df = pd.read_csv(population_file, sep='\\t',  header=None, names=['sample_id', 'population'])\n",
    "    pop_dict = dict(zip(pop_df['sample_id'], pop_df['population']))\n",
    "    # Convert genotypes to allele counts (0,1,2) and take every 5th variant\n",
    "    gn = gt.to_n_alt()[::5]  \n",
    "\n",
    "    # Transpose to get samples as rows (n_samples Ã— n_variants)\n",
    "    gn_t = gn.T\n",
    "    \n",
    "    # Standardize the data \n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    gn_std = scaler.fit_transform(gn_t)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(gn_std)\n",
    "    sample_ids = sample_metadata['samples']\n",
    "    populations = [pop_dict.get(sample_id, 'Unknown') for sample_id in sample_ids]\n",
    "    unique_pops = list(set(populations))\n",
    "    colors = [\"#4363d8\",\"#e6194b\",\"#3cb44b\",\"#f58231\",\"#911eb4\",\"#33b2b2\",\"#af2ca8\",\"#96c603\",\"#ca5d5d\",\"#008080\", \"#654977\",\"#29699E\",\"#8f862d\",\"#800000\",\"#4f9c66\",\"#808000\",\"#6e75b7\",\"#000075\",\"#662245\",\"#ff0000\", \"#DCD42E\", \"#44af69\", \"#f8333c\", \"#fcab10\", \"#2b9eb3\", \"#dbd5b5\", \"#ffdd19\"]\n",
    "    color_map = dict(zip(unique_pops, colors))\n",
    "    \n",
    "    # Plot first two PCs\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for pop in unique_pops:\n",
    "        mask = [p == pop for p in populations]\n",
    "        plt.scatter(pcs[mask, 0], pcs[mask, 1], \n",
    "                   c=[color_map[pop]], \n",
    "                   label=pop, \n",
    "                   alpha=1, \n",
    "                   s=70)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return pcs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MAF filtering\n",
    "maf_filtered_variants, maf_filtered_samples = filter_by_maf(variant_data, sample_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now we can finally run the PCA! Keep your eye out for distinctive clusters! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA \n",
    "pcs = run_pca(maf_filtered_variants, maf_filtered_samples, \"PATH\", n_components=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can do the GWAS, we will be visuallizing the results of our GWAS in a plot known as a Manhattan Plot; A Manhattan plot shows GWAS results as a skyline of p-values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_linregress(genotypes, phenotypes):\n",
    "    p_values = []\n",
    "    for SNP_index in range(genotypes.shape[0]): \n",
    "        x= genotypes[SNP_index, :]\n",
    "        if np.all(x == x[0]):\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "        results= stats.linregress(x,phenotypes)\n",
    "        p_values.append(results.pvalue) \n",
    "    \n",
    "    return p_values\n",
    "def run_gwas(variant_data: Dict, sample_metadata: Dict, phenotype_file: str):\n",
    "  \n",
    "    # Phenotype data \n",
    "    pheno_df = pd.read_csv(phenotype_file, sep='\\t',  header=None, names=['sample', 'phenotype'])\n",
    "    pheno_df = pheno_df.set_index('sample')\n",
    "    \n",
    "    #  Align genotypes with phenotypes\n",
    "    sample_ids = sample_metadata['samples']\n",
    "    phenotypes = pheno_df.loc[sample_ids]['phenotype'].values\n",
    "    \n",
    "    #  Convert genotypes to allele counts (0,1,2)\n",
    "    gt = allel.GenotypeArray(sample_metadata['genotypes'])\n",
    "    gn = gt.to_n_alt()  # shape: (n_variants, n_samples)\n",
    "    \n",
    "    #  Run regression\n",
    "    p_values = association_linregress(gn, phenotypes)\n",
    "    \n",
    "    # Create Manhattan plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    chrom = variant_data['CHROM']\n",
    "    pos = variant_data['POS']\n",
    "    \n",
    "    \n",
    "    # Convert chromosome names to numeric\n",
    "    unique_chroms = set(chrom)\n",
    "    def parse_chromosome_name(chrom_name):\n",
    "        chrom_part = chrom_name.replace('chr', '')  # Remove 'chr' prefix\n",
    "        main_part = chrom_part.split('_')[0]       # Some datasets with annotate their chromosomes with underscores!\n",
    "    \n",
    "        if main_part.isdigit():\n",
    "            return int(main_part)   \n",
    "        elif main_part == 'X':\n",
    "            return 23               \n",
    "        elif main_part == 'Y':\n",
    "            return 24  \n",
    "        elif main_part == 'MT' or main_part == 'M':\n",
    "            return 25  \n",
    "        else:\n",
    "            return 0 \n",
    "    chrom_num = [parse_chromosome_name(c) for c in chrom]\n",
    "    # Plotting Dataframe\n",
    "    gwas_df = pd.DataFrame({'CHR': chrom_num,'BP': pos,'P': p_values})\n",
    "    \n",
    "    # Manhattan plot \n",
    "    gwas_df = gwas_df.sort_values(['CHR', 'BP'])\n",
    "    gwas_df['ind'] = range(len(gwas_df))\n",
    "    gwas_df_grouped = gwas_df.groupby('CHR')\n",
    "    \n",
    "    colors = ['#fc0f0f', '#fc9a0f','#fcf80f', '#4efc0f', '#0fa5fc', '#c10ffc','#fc0fb5']  \n",
    "    x_labels = []\n",
    "    x_labels_pos = []\n",
    "    \n",
    "    for num, (name, group) in enumerate(gwas_df_grouped):\n",
    "        color = colors[num % len(colors)]\n",
    "        plt.scatter(group['ind'], -np.log10(group['P']), \n",
    "                   color=color, s=2)\n",
    "        x_labels.append(name)\n",
    "        x_labels_pos.append((group['ind'].iloc[-1] - (group['ind'].iloc[-1] - group['ind'].iloc[0])/2))\n",
    "    \n",
    "    plt.axhline(y=-np.log10(5e-8), color='r', linestyle='--')  # Genome-wide significance\n",
    "    plt.xticks(x_labels_pos, x_labels)\n",
    "    plt.xlabel('Chromosome')\n",
    "    plt.ylabel('-log10(p-value)')\n",
    "    plt.title('Manhattan Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return p_values, gwas_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values, gwas_results = run_gwas(maf_filtered_variants, maf_filtered_samples,\"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you guess what the phenotype was based on the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can save our work! This will likely take several minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_vcf(variant_data: dict, sample_metadata: dict, output_path: str):\n",
    "    \"\"\"\n",
    "    Fast VCF writer using manual construction for speed.\n",
    "    \"\"\"\n",
    "    samples = sample_metadata['samples']\n",
    "    genotypes = sample_metadata['genotypes']  # shape: (num_variants, num_samples, 2)\n",
    "\n",
    "    with gzip.open(output_path, 'wt') as f:\n",
    "        # Write VCF header\n",
    "        f.write(\"##fileformat=VCFv4.2\\n\")\n",
    "        f.write(\"##source=Generated by liftover pipeline\\n\")\n",
    "        for chrom in sorted(set(variant_data['CHROM'])):\n",
    "            chrom_str = f\"chr{chrom}\" if not str(chrom).startswith(\"chr\") else str(chrom)\n",
    "            f.write(f\"##contig=<ID={chrom_str}>\\n\")\n",
    "        f.write(\"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\")\n",
    "        f.write(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\t\")\n",
    "        f.write(\"\\t\".join(samples) + \"\\n\")\n",
    "\n",
    "        # Write variants\n",
    "        num_variants = len(variant_data['CHROM'])\n",
    "        for i in range(num_variants):\n",
    "            chrom = variant_data['CHROM'][i]\n",
    "            chrom_str = f\"chr{chrom}\" if not str(chrom).startswith(\"chr\") else str(chrom)\n",
    "            pos = variant_data['POS'][i]\n",
    "            vid = variant_data['ID'][i]\n",
    "            ref = variant_data['REF'][i]\n",
    "            alt = variant_data['ALT'][i] if variant_data['ALT'][i] not in ('.', None) else '.'\n",
    "            qual = str(variant_data['QUAL'][i]) if 'QUAL' in variant_data else '.'\n",
    "            filt = variant_data['FILTER'][i] if 'FILTER' in variant_data else '.'\n",
    "            if filt in (None, '.'):\n",
    "                filt = '.'\n",
    "            info = '.'\n",
    "            fmt = 'GT'\n",
    "\n",
    "            # Process genotypes as strings\n",
    "            gt_strings = []\n",
    "            for j in range(len(samples)):\n",
    "                gt = genotypes[i, j]\n",
    "                if gt[0] == -1 or gt[1] == -1:\n",
    "                    gt_str = './.'\n",
    "                else:\n",
    "                    gt_str = f\"{int(gt[0])}/{int(gt[1])}\"\n",
    "                gt_strings.append(gt_str)\n",
    "\n",
    "            line = f\"{chrom_str}\\t{pos}\\t{vid}\\t{ref}\\t{alt}\\t{qual}\\t{filt}\\t{info}\\t{fmt}\\t\" + \"\\t\".join(gt_strings)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "save_vcf(variant_data, sample_metadata, \"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ **You Did It! Look at You Go, Hot Shot!** ðŸŽ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benjie's Fun time Jupyter Hub",
   "language": "python",
   "name": "Benjie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
